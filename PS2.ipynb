{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#objective 1\n",
    "#extract text from FIR images \n",
    "\n",
    "# Generic Libraries\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re,string,unicodedata\n",
    "\n",
    "#Tesseract Library\n",
    "import pytesseract\n",
    "\n",
    "#Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Garbage Collection\n",
    "import gc\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pytesseract\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "custom_config = ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder containing the images\n",
    "folder_path = \"FIR_images_v1\"\n",
    "\n",
    "# Get the list of files in the folder\n",
    "image_files = os.listdir(folder_path)\n",
    "\n",
    "# Custom Tesseract configuration if needed\n",
    "custom_config = r'--oem 3 --psm 6'\n",
    "\n",
    "# Iterate over each image file\n",
    "for image_file in image_files:\n",
    "    # Construct the full path to the image\n",
    "    image_path = os.path.join(folder_path, image_file)\n",
    "    \n",
    "    # Load the image using OpenCV\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # Perform OCR using pytesseract\n",
    "    text = pytesseract.image_to_string(img, config=custom_config)\n",
    "\n",
    "    # the output of OCR can be saved in a file in necessary\n",
    "    file = open('output_final.txt','a') # file opened in append mode\n",
    "    file.write(text)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#objective 2\n",
    " \n",
    "# just to show how spacy works \n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load English tokenizer, tagger, parser and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process whole documents\n",
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual code for legal keyword recognization from extracted text \n",
    "\n",
    "import spacy\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warning\n",
    "warnings.filterwarnings(\"ignore\", message=\"Model 'en_blackstone_proto' .*\")\n",
    "\n",
    "try:\n",
    "    # Load English tokenizer, tagger, parser and NER\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except IOError as e:\n",
    "    print(f\"Error loading spaCy model: {e}\")\n",
    "    # Handle the error gracefully, such as loading an alternative model or exiting the program\n",
    "    exit(1)\n",
    "\n",
    "# Read input text file\n",
    "input_file_path = \"output.txt_final\"  # Change this to your input file path\n",
    "try:\n",
    "    with open(input_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "except UnicodeDecodeError:\n",
    "    print(\"UnicodeDecodeError: Unable to decode the file using UTF-8 encoding.\")\n",
    "    # Handle the decoding error gracefully, such as trying a different encoding or exiting the program\n",
    "    exit(1)\n",
    "\n",
    "# Chunk size for processing\n",
    "chunk_size = 100000  # Adjust as needed based on your text length and available memory\n",
    "\n",
    "# Split the text into chunks\n",
    "chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# Apply the model to each chunk\n",
    "for chunk in chunks:\n",
    "    doc = nlp(chunk)\n",
    "    \n",
    "    # Output file path for each chunk\n",
    "    output_file_path = \"output_chunk.txt\"  # Change this to your output file path for each chunk\n",
    "    \n",
    "    # Write the entities identified by the model to the output file for each chunk\n",
    "    with open(output_file_path, \"a\", encoding=\"utf-8\") as output_file:\n",
    "        for ent in doc.ents:\n",
    "            output_file.write(f\"{ent.text} - {ent.label_}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective 3\n",
    "# classifying extracted legal keywords into crime categories \n",
    " \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FIR Classification and Punishment Recommendation\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read FIR data from input file\n",
    "# Read FIR data from input file\n",
    "input_file_path = \"C:\\\\Users\\\\akank\\\\OneDrive\\\\Desktop\\\\ML March\\\\output_chunk.txt\"  # Replace with your input file path\n",
    "fir_data = spark.read.text(input_file_path)\n",
    "\n",
    "\n",
    "# Function to classify criminal acts based on identified IPC sections\n",
    "def classify_criminal_act(description):\n",
    "    # Add your logic to classify criminal acts based on the description and identified IPC sections\n",
    "    # Example:\n",
    "    if \"theft\" in description.lower():\n",
    "        return \"Theft\"\n",
    "    elif \"assault\" in description.lower():\n",
    "        return \"Assault\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "# Function to recommend potential punishments based on classified acts and relevant IPC sections\n",
    "def recommend_punishment(criminal_act):\n",
    "    # Add your logic to recommend potential punishments based on the classified act and relevant IPC sections\n",
    "    # Example:\n",
    "    if criminal_act == \"Theft\":\n",
    "      return \"Imprisonment up to 3 years or fine or both (IPC Section 379)\"\n",
    "    elif criminal_act == \"Assault\":\n",
    "        return \"Imprisonment up to 1 year or fine or both (IPC Section 352)\"\n",
    "    else:\n",
    "        return \"Punishment not specified\"\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Define the classification function as a UDF\n",
    "classify_criminal_act_udf = udf(classify_criminal_act, StringType())\n",
    "\n",
    "# Apply classification function to FIR data\n",
    "classified_fir_data = fir_data.withColumn(\"Criminal_Act\", classify_criminal_act_udf(fir_data[\"value\"]))\n",
    "\n",
    "# Output the results\n",
    "classified_fir_data.show(truncate=False)\n",
    "\n",
    "\n",
    "# Output the results\n",
    "classified_fir_data.show(truncate=False)\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
